{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# DINOv2 model fine-tuning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd39527e0ca5e05"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This notebook describe the process of fine-tuning DINOv2 model on a modified ImageNet-100 dataset (ImageNet-200)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae0d6f045a9fb28d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc2bf432b299f0a2"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Ignore warning while running the code\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T05:08:52.330731Z",
     "start_time": "2024-04-17T05:08:52.323630Z"
    }
   },
   "id": "f87c805eb3e8c9a4"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Handling path\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# DINOv2 ViT model\n",
    "from dinov2.models.vision_transformer import vit_small"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T05:08:53.710283Z",
     "start_time": "2024-04-17T05:08:52.329692Z"
    }
   },
   "id": "a1b3791a20d9a59a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset path"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb1504e302db5324"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Firstly, populate the train and validation dataset by creating two folders, one named train and one named val. In each of the folder, group images based on their classes and put them in the folder with name of the class."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6512a659c6a7949f"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Define the local directory, as well as the path to training and validation set\n",
    "local_directory = os.getcwd()\n",
    "train_dataset_dir = Path(\"../../data/train\")\n",
    "valid_dataset_dir = Path(\"../../data/val\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T05:12:01.756999Z",
     "start_time": "2024-04-17T05:12:01.755430Z"
    }
   },
   "id": "cb550781111730fb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Image Resizing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d171f01ad172b06"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we define resizing method to make sure that the size of the image fit with our model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43bec6c97437c1e2"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class ResizeAndPad:\n",
    "    def __init__(self, target_size, multiple):\n",
    "        \"\"\"\n",
    "        Helper class to perform resize and padding on the image\n",
    "        \"\"\"\n",
    "        self.target_size = target_size\n",
    "        self.multiple = multiple\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Call transformation on the image\n",
    "        \"\"\"\n",
    "        # Resize the image\n",
    "        img = transforms.Resize(self.target_size)(img)\n",
    "\n",
    "        # Calculate padding\n",
    "        pad_width = (self.multiple - img.width % self.multiple) % self.multiple\n",
    "        pad_height = (self.multiple - img.height % self.multiple) % self.multiple\n",
    "\n",
    "        # Apply padding\n",
    "        img = transforms.Pad(\n",
    "            (pad_width // 2, \n",
    "             pad_height // 2, \n",
    "             pad_width - pad_width // 2, \n",
    "             pad_height - pad_height // 2)\n",
    "        )(img)\n",
    "        \n",
    "        return img"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T05:15:22.988051Z",
     "start_time": "2024-04-17T05:15:22.981330Z"
    }
   },
   "id": "f0eff55a81e1e789"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Define supported image size\n",
    "IMAGE_SIZE = 256\n",
    "TARGET_SIZE = (IMAGE_SIZE, IMAGE_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T05:20:24.652766Z",
     "start_time": "2024-04-17T05:20:24.642541Z"
    }
   },
   "id": "167234b0aa57d40d"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Define the DATA TRANSFORMATION process that images have to go through\n",
    "DATA_TRANSFORM = {\n",
    "    \"train\": transforms.Compose(\n",
    "        [\n",
    "            ResizeAndPad(TARGET_SIZE, 14),\n",
    "            transforms.RandomRotation(360),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    ),\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T05:21:31.659751Z",
     "start_time": "2024-04-17T05:21:31.655374Z"
    }
   },
   "id": "54e9d47c8751d3f9"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Define the DATASETS, DATALOADERS and CLASSNAME\n",
    "DATASETS = {\n",
    "    \"train\": datasets.ImageFolder(train_dataset_dir, DATA_TRANSFORM[\"train\"])\n",
    "}\n",
    "\n",
    "DATALOADERS = {\n",
    "    \"train\": torch.utils.data.DataLoader(DATASETS[\"train\"], batch_size=8, shuffle=True)\n",
    "}\n",
    "\n",
    "CLASSES = DATASETS[\"train\"].classes"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T05:24:17.320558Z",
     "start_time": "2024-04-17T05:24:17.318582Z"
    }
   },
   "id": "61ae9f02f80b1f5c"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Define the DEVICE for training the model\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T05:25:12.965066Z",
     "start_time": "2024-04-17T05:25:12.960592Z"
    }
   },
   "id": "26f3ffecbf1ec114"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DINOv2 Classification Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d42f162e5ff90d72"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DINOClassificationModel(nn.Module):\n",
    "    def __index__(self):\n",
    "        \"\"\"\n",
    "        Load the pretrained DINOv2 Classification Model\n",
    "        \"\"\"\n",
    "        super(DINOClassificationModel, self).__init__()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79102445e5aabbc7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
