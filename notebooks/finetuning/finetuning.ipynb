{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# DINOv2 model fine-tuning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd39527e0ca5e05"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This notebook describe the process of fine-tuning DINOv2 model on a modified ImageNet-100 dataset (ImageNet-200)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae0d6f045a9fb28d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc2bf432b299f0a2"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Ignore warning while running the code\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T04:53:01.515027Z",
     "start_time": "2024-04-18T04:53:01.510163Z"
    }
   },
   "id": "f87c805eb3e8c9a4"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Handling path\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "from copy import deepcopy\n",
    "\n",
    "# DINOv2 ViT model\n",
    "from dinov2.models.vision_transformer import vit_small\n",
    "\n",
    "# Timing\n",
    "import time\n",
    "\n",
    "# Numpy \n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T04:53:03.927365Z",
     "start_time": "2024-04-18T04:53:02.207596Z"
    }
   },
   "id": "a1b3791a20d9a59a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset path"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb1504e302db5324"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Firstly, populate the train and validation dataset by creating two folders, one named train and one named val. In each of the folder, group images based on their classes and put them in the folder with name of the class."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6512a659c6a7949f"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Define the local directory, as well as the path to training and validation set\n",
    "local_directory = os.getcwd()\n",
    "train_dataset_dir = Path(\"../../data/train\")\n",
    "valid_dataset_dir = Path(\"../../data/val\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T04:53:06.059963Z",
     "start_time": "2024-04-18T04:53:06.055199Z"
    }
   },
   "id": "cb550781111730fb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Image Resizing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d171f01ad172b06"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we define resizing method to make sure that the size of the image fit with our model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43bec6c97437c1e2"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class ResizeAndPad:\n",
    "    def __init__(self, target_size, multiple):\n",
    "        \"\"\"\n",
    "        Helper class to perform resize and padding on the image\n",
    "        \"\"\"\n",
    "        self.target_size = target_size\n",
    "        self.multiple = multiple\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Call transformation on the image\n",
    "        \"\"\"\n",
    "        # Resize the image\n",
    "        img = transforms.Resize(self.target_size)(img)\n",
    "\n",
    "        # Calculate padding\n",
    "        pad_width = (self.multiple - img.width % self.multiple) % self.multiple\n",
    "        pad_height = (self.multiple - img.height % self.multiple) % self.multiple\n",
    "\n",
    "        # Apply padding\n",
    "        img = transforms.Pad(\n",
    "            (pad_width // 2, \n",
    "             pad_height // 2, \n",
    "             pad_width - pad_width // 2, \n",
    "             pad_height - pad_height // 2)\n",
    "        )(img)\n",
    "        \n",
    "        return img"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T04:53:11.668186Z",
     "start_time": "2024-04-18T04:53:11.664787Z"
    }
   },
   "id": "f0eff55a81e1e789"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Define supported image size\n",
    "IMAGE_SIZE = 256\n",
    "TARGET_SIZE = (IMAGE_SIZE, IMAGE_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T04:53:12.340836Z",
     "start_time": "2024-04-18T04:53:12.333747Z"
    }
   },
   "id": "167234b0aa57d40d"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Define the DATA TRANSFORMATION process that images have to go through\n",
    "DATA_TRANSFORM = {\n",
    "    \"train\": transforms.Compose(\n",
    "        [\n",
    "            ResizeAndPad(TARGET_SIZE, 14),\n",
    "            transforms.RandomRotation(360),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    ),\n",
    "    \"valid\": transforms.Compose(\n",
    "        [\n",
    "            ResizeAndPad(TARGET_SIZE, 14),\n",
    "            transforms.RandomRotation(360),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    ),\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T04:53:13.040366Z",
     "start_time": "2024-04-18T04:53:13.026844Z"
    }
   },
   "id": "54e9d47c8751d3f9"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Define the DATASETS, DATALOADERS and CLASSNAME\n",
    "DATASETS = {\n",
    "    \"train\": datasets.ImageFolder(train_dataset_dir, DATA_TRANSFORM[\"train\"]),\n",
    "    \"valid\": datasets.ImageFolder(valid_dataset_dir, DATA_TRANSFORM[\"valid\"])\n",
    "}\n",
    "\n",
    "DATALOADERS = {\n",
    "    \"train\": torch.utils.data.DataLoader(DATASETS[\"train\"], batch_size=8, shuffle=True),\n",
    "    \"valid\": torch.utils.data.DataLoader(DATASETS[\"valid\"], batch_size=8, shuffle=True)\n",
    "}\n",
    "\n",
    "CLASSES = DATASETS[\"train\"].classes"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T04:53:14.226531Z",
     "start_time": "2024-04-18T04:53:14.220324Z"
    }
   },
   "id": "61ae9f02f80b1f5c"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Define the DEVICE for training the model\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T04:53:14.530623Z",
     "start_time": "2024-04-18T04:53:14.526195Z"
    }
   },
   "id": "26f3ffecbf1ec114"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DINOv2 Classification Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d42f162e5ff90d72"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class DINOClassificationModel(nn.Module):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        \"\"\"\n",
    "        Load the pretrained DINOv2 Classification Model\n",
    "        \"\"\"\n",
    "        # Initialize module\n",
    "        super(DINOClassificationModel, self).__init__()\n",
    "        \n",
    "        # Load model with register\n",
    "        model = vit_small(patch_size=14,\n",
    "                          img_size=526,\n",
    "                          init_values=1.0,\n",
    "                          num_register_tokens=4,\n",
    "                          block_chunks=0)\n",
    "        self.embedding_size = 384\n",
    "        self.number_of_heads = 6\n",
    "        self.num_classes = num_classes\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Load the pre-trained weights\n",
    "        model.load_state_dict(\n",
    "            torch.load(\n",
    "                \"../../pretrained/dinov2_vits14_reg4_pretrain.pth\"\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Copy the model\n",
    "        self.transformers = deepcopy(model)\n",
    "        \n",
    "        # Add the classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.embedding_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, self.num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward the inputs\n",
    "        inputs: tensor of size (batch_size, image_height, image_width, channels)\n",
    "        \"\"\"\n",
    "        # Pass through the transformers and normalization\n",
    "        outputs = self.transformers(inputs)\n",
    "        outputs = self.transformers.norm(outputs)\n",
    "        outputs = self.classifier(outputs)\n",
    "        return outputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T04:55:13.238904Z",
     "start_time": "2024-04-18T04:55:13.234366Z"
    }
   },
   "id": "79102445e5aabbc7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create trainer and train function for the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "982f16b34f41f4e4"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, device, train_loader, val_loader, args):\n",
    "        \"\"\"\n",
    "        Initialize the trainer for the DINOv2 ViT model\n",
    "        \"\"\"\n",
    "        # Cache the parameters\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.args = args\n",
    "        \n",
    "        # Model to device\n",
    "        self.model.to(device)\n",
    "        \n",
    "        # Create optimizer and cross-entropy loss function\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), args[\"lr\"])\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def train(self, epoch):\n",
    "        \"\"\"\n",
    "        Train the Visual Transformer for one epoch\n",
    "        :param epoch: the current epoch\n",
    "        :return: epoch loss and accuracy\n",
    "        \"\"\"\n",
    "        # Get the current time\n",
    "        current_time = time.time()\n",
    "\n",
    "        # Get the number of batches and the number of samples of the test loader\n",
    "        n_batches, n_samples = len(self.train_loader), len(self.train_loader.dataset)\n",
    "\n",
    "        # Initialize the loss and accuracy\n",
    "        epoch_loss = 0.0\n",
    "        epoch_accuracy = 0.0\n",
    "\n",
    "        # Put the model into train mode\n",
    "        self.model.train()\n",
    "\n",
    "        # Calculate the loss and accuracy\n",
    "        for image, label in self.train_loader:\n",
    "            # print(\"PROCESSING\")\n",
    "            # Map image and label to device\n",
    "            image = image.to(self.device)\n",
    "            label = label.to(self.device)\n",
    "\n",
    "            # Forward pass through visual transformer\n",
    "            output = self.model(image)\n",
    "            loss = self.criterion(output, label)\n",
    "\n",
    "            # Backward pass through visual transformer\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Calculate the loss and accuracy\n",
    "            acc = (output.argmax(dim=1) == label).float().sum()\n",
    "            epoch_accuracy += acc.item()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Calculate the loss and accuracy\n",
    "        epoch_loss = epoch_loss / n_batches\n",
    "        epoch_accuracy = epoch_accuracy / n_samples * 100\n",
    "\n",
    "        # Calculate the training time\n",
    "        print(time.time() - current_time)\n",
    "\n",
    "        # Display the current status\n",
    "        print('Train Epoch: {}\\t>\\tLoss: {:.4f} / Acc: {:.1f}%'.format(epoch, epoch_loss, epoch_accuracy))\n",
    "\n",
    "        return epoch_accuracy, epoch_accuracy\n",
    "    \n",
    "    def validate(self, epoch):\n",
    "        \"\"\"\n",
    "        Perform the validation at epoch\n",
    "        :param epoch: the current epoch\n",
    "        :return: the epoch loss and accuracy\n",
    "        \"\"\"\n",
    "        # Get the number of batches and the number of samples of the test loader\n",
    "        n_batches, n_samples = len(self.val_loader), len(self.val_loader.dataset)\n",
    "\n",
    "        # Put the model into eval mode\n",
    "        self.model.eval()\n",
    "\n",
    "        # Validate\n",
    "        with torch.no_grad():\n",
    "            epoch_val_accuracy = 0.0\n",
    "            epoch_val_loss = 0.0\n",
    "\n",
    "            for data, label in self.val_loader:\n",
    "                # Map image and label to device\n",
    "                data = data.to(self.device)\n",
    "                label = label.to(self.device)\n",
    "\n",
    "                # Forward pass through the Visual Transformer\n",
    "                val_output = self.model(data)\n",
    "                val_loss = self.criterion(val_output, label)\n",
    "\n",
    "                # Get the loss and accuracy\n",
    "                acc = (val_output.argmax(dim=1) == label).float().sum()\n",
    "                epoch_val_accuracy += acc.item()\n",
    "                epoch_val_loss += val_loss.item()\n",
    "\n",
    "        # Calculate the validation accuracy and loss\n",
    "        epoch_val_loss = epoch_val_loss / n_batches\n",
    "        epoch_val_accuracy = epoch_val_accuracy / n_samples * 100\n",
    "\n",
    "        # Display the current stats\n",
    "        print('Validation Epoch: {}\\t>\\tLoss: {:.4f} / Acc: {:.1f}%'.format(epoch, epoch_val_loss,epoch_val_accuracy))\n",
    "        \n",
    "        return epoch_val_loss, epoch_val_accuracy\n",
    "    \n",
    "    def save(self, model_path, epoch):\n",
    "        \"\"\"\n",
    "        Save the current model\n",
    "        :param model_path: the saved model path\n",
    "        :param epoch: the current epoch\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        }, model_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T04:55:13.726463Z",
     "start_time": "2024-04-18T04:55:13.722316Z"
    }
   },
   "id": "2722d96e7c781bb7"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# Create a train function\n",
    "def train(model, datasets, dataloaders, args, device):\n",
    "    \"\"\"\n",
    "    args: training arguments\n",
    "    \"\"\"\n",
    "    # Display info\n",
    "    print('Train dataset of size %d' % len(datasets[\"train\"]))\n",
    "    print('Validation dataset of size %d' % len(datasets[\"valid\"]))\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        device,\n",
    "        dataloaders[\"train\"],\n",
    "        dataloaders[\"valid\"],\n",
    "        args\n",
    "    )\n",
    "    \n",
    "    # Start training\n",
    "    val_loss_array = []\n",
    "    train_loss_array = []\n",
    "    val_accuracy_array = []\n",
    "    train_accuracy_array = []\n",
    "    \n",
    "    # Model save directory\n",
    "    stats_save_dir = args[\"save_dir\"]\n",
    "    stats_save_address = stats_save_dir + '/results.npy'\n",
    "    \n",
    "    # Train & Validate\n",
    "    for epoch in range(1, args[\"epochs\"] + 1):\n",
    "        # Train the model for thi epoch\n",
    "        epoch_loss, epoch_accuracy = trainer.train(epoch)\n",
    "\n",
    "        # Validate the model\n",
    "        epoch_val_loss, epoch_val_accuracy = trainer.validate(epoch)\n",
    "\n",
    "        # Save the model\n",
    "        trainer.save(args[\"output_model_prefix\"], epoch)\n",
    "\n",
    "        # Save the training and validation accuracy\n",
    "        val_accuracy_array.append(epoch_val_accuracy)\n",
    "        train_accuracy_array.append(epoch_accuracy)\n",
    "\n",
    "        # Save the validation and training loss\n",
    "        val_loss_array.append(epoch_val_loss)\n",
    "        train_loss_array.append(epoch_loss)\n",
    "\n",
    "        # Save the training and validation result\n",
    "        losses = np.asarray([train_loss_array, val_loss_array, train_accuracy_array, val_accuracy_array])\n",
    "        np.save(stats_save_address, losses)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T04:55:14.066189Z",
     "start_time": "2024-04-18T04:55:14.060657Z"
    }
   },
   "id": "73fedcde862bd41d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca389b57e7b69d6b"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# Define arguments\n",
    "ARGS = {\n",
    "    \"lr\": 10e-6,\n",
    "    \"save_dir\": \"./models\",\n",
    "    \"output_model_prefix\": \"./models/model.pth\",\n",
    "    \"epochs\": 50,\n",
    "    \"hidden_size\": 256,\n",
    "    \"num_classes\": 2\n",
    "}\n",
    "\n",
    "# Create model\n",
    "model = DINOClassificationModel(hidden_size=ARGS[\"hidden_size\"], num_classes=ARGS[\"num_classes\"])\n",
    "\n",
    "# Create training pipeline\n",
    "train(\n",
    "    model=model,\n",
    "    datasets=DATASETS,\n",
    "    dataloaders=DATALOADERS,\n",
    "    args=ARGS,\n",
    "    device=DEVICE\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T05:00:29.241154Z",
     "start_time": "2024-04-18T05:00:29.225182Z"
    }
   },
   "id": "3bec90355913ef17"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
